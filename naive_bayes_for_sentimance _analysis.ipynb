{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SENTIMENT ANLAYSIS WITH NAIVE BAYES \n",
    "This code cell imports the necessary libraries and functions needed for processing and analyzing a sample Twitter dataset for sentiment analysis:\n",
    "\n",
    "- `twitter_samples` from NLTK: Provides a sample Twitter dataset.\n",
    "- `clean_text`, `process_tweet`, `build_freqs` from `helper_function`: Custom helper functions to clean and process tweets and build frequency distributions.\n",
    "- `numpy` as `np`: A fundamental package for scientific computing with Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "from helper_funtion import clean_text,process_tweet,build_freqs\n",
    "import numpy as np    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the set of positive tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')  # Load positive tweets from 'positive_tweets.json' file\n",
    "\n",
    "# Select the set of negative tweets\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')  # Load negative tweets from 'negative_tweets.json' file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(all_positive_tweets))\n",
    "print(len(all_negative_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code splits the dataset of positive and negative tweets into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# avoid assumptions about the length of all_positive_tweets\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@BhaktisBanter @PallaviRuhail This one is irresistible :)\\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEANING THE TWEET\n",
    "\n",
    "This code splits the dataset of positive and negative tweets into training and testing sets. Then it cleans the text of the training and test datasets using the `clean_text` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text in the training set\n",
    "for i in range(len(train_x)):\n",
    "    train_x[i] = clean_text(train_x[i])  # Apply the clean_text function to each tweet in the training set\n",
    "\n",
    "# Clean the text in the test set\n",
    "for i in range(len(test_x)):\n",
    "    test_x[i] = clean_text(test_x[i])  # Apply the clean_text function to each tweet in the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'followfriday franceint pkuchli milipolpari top engag member commun week'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `process_tweet` function tokenizes each line of tweet into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_train_x=process_tweet(train_x)\n",
    "pro_test_y=process_tweet(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['followfriday',\n",
       " 'franceint',\n",
       " 'pkuchli',\n",
       " 'milipolpari',\n",
       " 'top',\n",
       " 'engag',\n",
       " 'member',\n",
       " 'commun',\n",
       " 'week']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_train_x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "building frequency from train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('followfriday', 1.0) 23\n",
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 16527\n"
     ]
    }
   ],
   "source": [
    "freqs=build_freqs(pro_train_x, train_y)\n",
    "first_key = next(iter(freqs))\n",
    "print(first_key, freqs[first_key])\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(freqs, word, label):\n",
    "    \"\"\"\n",
    "    Given a dictionary of frequencies, a word, and a label (1 for positive, 0 for negative),\n",
    "    return the count of the (word, label) pair.\n",
    "    \"\"\"\n",
    "    return freqs.get((word, label), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs,train_x,train_y):\n",
    "    '''here first gonna get prob of pos word and neg word'''\n",
    "\n",
    "\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos, N_neg, V_pos, V_neg\n",
    "    N_pos = N_neg = V_pos = V_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        if pair[1]>0:\n",
    "            # increment the count of unique positive words by 1\n",
    "            V_pos+=1\n",
    "            # increment the number of positive words by the count for this (word,label) pair\n",
    "            N_pos += freqs[pair]\n",
    "        else:\n",
    "            # increment the count of unique negative words by 1\n",
    "            V_neg += 1\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "\n",
    "    D = len(train_y)\n",
    "\n",
    "    D_pos = (len(list(filter(lambda x: x > 0, train_y))))\n",
    "    D_neg = (len(list(filter(lambda x: x <= 0, train_y))))\n",
    "\n",
    "     # Calculate logprior\n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "    for word in vocab:\n",
    "\n",
    "        #   get the positive and negative frequency of the word\n",
    "        freq_pos = lookup(freqs,word,1)\n",
    "        freq_neg = lookup(freqs,word,0)\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "14308\n"
     ]
    }
   ],
   "source": [
    "logprior, loglikelihood = train_naive_bayes(freqs, pro_train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Preprocess the tweet by tokenizing and normalizing.\n",
    "    This function can be adjusted based on the preprocessing steps used during training.\n",
    "    \"\"\"\n",
    "    # Tokenize the tweet into words (simple split by spaces, can be enhanced)\n",
    "    wordss=clean_text(tweet)\n",
    "    words=wordss.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT  THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_model(tweet,logprior,loglikelihood):\n",
    "    words = preprocess_tweet(tweet)\n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "\n",
    "    for word in words:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if predict_by_model(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.mean(np.absolute(y_hats-test_y))\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1-error\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.7700\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 2.08\n",
      "I am bad -> -1.24\n",
      "this movie should have been great. -> 1.97\n",
      "great -> 2.08\n",
      "great great -> 4.16\n",
      "great great great -> 6.23\n",
      "great great great great -> 8.31\n"
     ]
    }
   ],
   "source": [
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    # print( '%s -> %f' % (tweet, naive_bayes_predict(tweet, logprior, loglikelihood)))\n",
    "    p = predict_by_model(tweet, logprior, loglikelihood)\n",
    "#     print(f'{tweet} -> {p:.2f} ({p_category})')\n",
    "    print(f'{tweet} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example :1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2387204058495098"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tweet = 'you are bad :('\n",
    "predict_by_model(my_tweet, logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is -3.0202233749103757\n"
     ]
    }
   ],
   "source": [
    "tweet='i deeply sad'\n",
    "p = predict_by_model(tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)\n",
    "# p = test_the_model(tweet, logprior, loglikelihood)\n",
    "# print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### geting ratio  of postive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(freqs, word):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary containing the words\n",
    "\n",
    "    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n",
    "        Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "    '''\n",
    "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # use lookup() to find positive counts for the word (denoted by the integer 1)\n",
    "    pos_neg_ratio['positive'] = lookup(freqs,word,1)\n",
    "\n",
    "    # use lookup() to find negative counts for the word (denoted by integer 0)\n",
    "    pos_neg_ratio['negative'] = lookup(freqs,word,0)\n",
    "\n",
    "    # calculate the ratio of positive to negative counts for the word\n",
    "    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive'] + 1)/(pos_neg_ratio['negative'] + 1)\n",
    "    ### END CODE HERE ###\n",
    "    return pos_neg_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 31, 'negative': 42, 'ratio': 0.7441860465116279}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_ratio(freqs, 'never')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3276870378252548\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'I am happy because I am progressing :)'\n",
    "\n",
    "p = predict_by_model(my_tweet, logprior, loglikelihood)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.7229682981668655\n"
     ]
    }
   ],
   "source": [
    "my_tweet=\"hate story\"\n",
    "p = predict_by_model(my_tweet, logprior, loglikelihood)\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

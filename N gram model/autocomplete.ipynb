{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\8086f\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and inspecting the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 3335477\n",
      "First 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Last 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\\nColombia is with an 'o'...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\\n#GutsiestMovesYouCanMake Giving a cat a bath.\\nCoffee after 5 was a TERRIBLE idea.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "with open(\"en_US.twitter.txt\", \"r\",encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "print(\"Data type:\", type(data))\n",
    "print(\"Number of letters:\", len(data))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[0:300])\n",
    "print(\"-------\")\n",
    "\n",
    "print(\"Last 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[-300:])\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    "    \"\"\"\n",
    "    Split data by linebreak \"\\n\"\n",
    "    \n",
    "    Args:\n",
    "        data: str\n",
    "    \n",
    "    Returns:\n",
    "        A list of sentences\n",
    "    \"\"\"\n",
    "    sentences = data.split('\\n')\n",
    "    \n",
    "    # Additional cleaning\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    \n",
    "    return sentences \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am praticing n gram', 'i have greater ideas on NLP']\n"
     ]
    }
   ],
   "source": [
    "new_corpus=split_to_sentences(\"i\\n am praticing n gram \\n i have greater ideas on NLP\")\n",
    "print(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Tokenize sentences into tokens (words)\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of strings\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        # Convert to lowercase letters\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        # Convert into a list of words\n",
    "        tokenized = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        # append the list of words to the list of lists\n",
    "        tokenized_sentences.append(tokenized)\n",
    "\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['heyy', 'whatsss', 'up', '?'], ['i', 'am', 'happy']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examp=['heyy whatsss up? ',' i am happy']\n",
    "tokenize_sentences(examp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(corpus):\n",
    "\n",
    "    sentence=split_to_sentences(corpus)\n",
    "    \n",
    "    sentence = tokenize_sentences(sentence)\n",
    "\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\n",
      "Colombia is with an 'o'...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\n",
      "#GutsiestMovesYouCanMake Giving a cat a bath.\n",
      "Coffee after 5 was a TERRIBLE idea.\n",
      "\n",
      "[['ust', 'had', 'one', 'a', 'few', 'weeks', 'back', '....', 'hopefully', 'we', 'will', 'be', 'back', 'soon', '!', 'wish', 'you', 'the', 'best', 'yo'], ['colombia', 'is', 'with', 'an', \"'\", 'o', \"'\", '...', '“', ':', 'we', 'now', 'ship', 'to', '4', 'countries', 'in', 'south', 'america', '(', 'fist', 'pump', ')', '.', 'please', 'welcome', 'columbia', 'to', 'the', 'stunner', 'family', '”'], ['#', 'gutsiestmovesyoucanmake', 'giving', 'a', 'cat', 'a', 'bath', '.'], ['coffee', 'after', '5', 'was', 'a', 'terrible', 'idea', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(data[-300:])\n",
    "eg=tokenize_data(data[-300:])\n",
    "print(eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data=tokenize_data(data)\n",
    "\n",
    "random.seed(87)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_vocab(tokenized_data):\n",
    "    word_count={}\n",
    "    for sentence in tokenized_data:\n",
    "        for word in sentence:\n",
    "            word_count[word]=word_count.get(word,0) +1\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sky': 1,\n",
       " 'is': 1,\n",
       " 'blue': 1,\n",
       " '.': 3,\n",
       " 'leaves': 1,\n",
       " 'are': 2,\n",
       " 'green': 1,\n",
       " 'roses': 1,\n",
       " 'red': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
    "                       ['leaves', 'are', 'green', '.'],\n",
    "                       ['roses', 'are', 'red', '.']]\n",
    "creating_vocab(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 18500\n",
      "personally 16\n",
      "would 1154\n",
      "like 2550\n"
     ]
    }
   ],
   "source": [
    "newvocab=creating_vocab(tokenized_data)\n",
    "i=0\n",
    "for word , key in newvocab.items():\n",
    "    i+=1\n",
    "    if i==5:  \n",
    "        break\n",
    "    print(word,key)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test your code\n",
    "# tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
    "#                        ['leaves', 'are', 'green', '.'],\n",
    "#                        ['roses', 'are', 'red', '.']]\n",
    "# tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\n",
    "# print(f\"Closed vocabulary:\")\n",
    "# print(tmp_closed_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing out of voacbulary word as unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_out_vocab_words(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    \"\"\"Replacing the unk words as unknown_token\n",
    "    \n",
    "    Keyword arguments:\n",
    "    tokenized_sentences -- description\n",
    "    Return: return_description\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    vocabulary=set(vocabulary)\n",
    "    replaced_tokenized_sentences=[]\n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_words=[]\n",
    "        for word in sentence:\n",
    "            if word in vocabulary:\n",
    "                replaced_words.append(word)\n",
    "            else:\n",
    "                replaced_words.append(unknown_token)\n",
    "\n",
    "        replaced_tokenized_sentences.append(replaced_words)       \n",
    "\n",
    "    return replaced_tokenized_sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sky', 'is', 'blue', '.'],\n",
       " ['leaves', 'are', 'green', '.'],\n",
       " ['roses', 'are', '<unk>', '.']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
    "                       ['leaves', 'are', 'green', '.'],\n",
    "                       ['roses', 'are', 'magenta', '.']]\n",
    "replace_out_vocab_words(tokenized_sentences,newvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
    "    \"\"\"\n",
    "    Find the words that appear N times or more\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of sentences\n",
    "        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.\n",
    "    \n",
    "    Returns:\n",
    "        List of words that appear N times or more\n",
    "    \"\"\"\n",
    "    \n",
    "    closed_vocab = []\n",
    "    \n",
    "    \n",
    "    word_counts = creating_vocab(tokenized_sentences)\n",
    "    # for each word and its count\n",
    "    for word, cnt in word_counts.items(): # complete this line\n",
    "        \n",
    "        # check that the word's count\n",
    "        # is at least as great as the minimum count\n",
    "        if cnt >= count_threshold:\n",
    "            \n",
    "            # append the word to the list\n",
    "            closed_vocab.append(word)\n",
    "   \n",
    "    \n",
    "    return closed_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed vocabulary:\n",
      "['.', 'are']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
    "                       ['leaves', 'are', 'green', '.'],\n",
    "                       ['roses', 'are', 'red', '.']]\n",
    "tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\n",
    "print(f\"Closed vocabulary:\")\n",
    "print(tmp_closed_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, count_threshold):\n",
    "    vocabulary = get_words_with_nplus_frequency(train_data,count_threshold)\n",
    "    train_data_replaced = replace_out_vocab_words(train_data,vocabulary)\n",
    "    test_data_replaced = replace_out_vocab_words(test_data,vocabulary)\n",
    "\n",
    "    return train_data_replaced, test_data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_train_repl\n",
      "[['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']]\n",
      "\n",
      "tmp_test_repl\n",
      "[['<unk>', 'are', '<unk>', '.']]\n",
      "\n",
      "tmp_vocab\n",
      "['sky', 'is', 'blue', '.', 'leaves', 'are', 'green']\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "tmp_train = [['sky', 'is', 'blue', '.'],\n",
    "     ['leaves', 'are', 'green']]\n",
    "tmp_test = [['roses', 'are', 'red', '.']]\n",
    "\n",
    "tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train, \n",
    "                                                           tmp_test, \n",
    "                                                           count_threshold = 1)\n",
    "\n",
    "print(\"tmp_train_repl\")\n",
    "print(tmp_train_repl)\n",
    "print()\n",
    "print(\"tmp_test_repl\")\n",
    "print(tmp_test_repl)\n",
    "print()\n",
    "print(\"tmp_vocab\")\n",
    "print(tmp_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n",
    "                                                                        test_data, \n",
    "                                                                        minimum_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creat N gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_gram(data, n, start_token='<s>', end_token = '<e>'):\n",
    "\n",
    "    ngrams={}\n",
    "\n",
    "    for sentence in data:\n",
    "        sentence= [start_token]*n+ sentence + [end_token]\n",
    "        \n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        m = len(sentence) if n==1 else len(sentence)-1\n",
    "        for i in range(m):\n",
    "            n_gram = sentence[i:i+n]\n",
    "            ngrams[n_gram]=ngrams.get(n_gram,0)+1\n",
    "    return ngrams\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
      "bi gram\n",
      "\n",
      "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n",
      "tri gram\n",
      "\n",
      "{('<s>', '<s>', '<s>'): 2, ('<s>', '<s>', 'i'): 1, ('<s>', 'i', 'like'): 1, ('i', 'like', 'a'): 1, ('like', 'a', 'cat'): 2, ('a', 'cat', '<e>'): 2, ('cat', '<e>'): 2, ('<s>', '<s>', 'this'): 1, ('<s>', 'this', 'dog'): 1, ('this', 'dog', 'is'): 1, ('dog', 'is', 'like'): 1, ('is', 'like', 'a'): 1}\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "print(\"Uni-gram:\")\n",
    "print(count_n_gram(sentences, 1))\n",
    "print(\"bi gram\\n\")\n",
    "print(count_n_gram(sentences, 2))\n",
    "print(\"tri gram\\n\")\n",
    "print(count_n_gram(sentences, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_matrix(data, n, start_token='<s>', end_token='<e>'):\n",
    "    ngrams = count_n_gram(data, n, start_token, end_token)\n",
    "    all_tag = sorted(set([gram[0] for gram in ngrams.keys()] + [gram[1] for gram in ngrams.keys()]))\n",
    "    numtag = len(all_tag)\n",
    "    A = np.zeros((numtag, numtag))\n",
    "    tag_to_index = {tag: index for index, tag in enumerate(all_tag)}\n",
    "\n",
    "    for (prev_tag, next_tag), count in ngrams.items():\n",
    "        if prev_tag in tag_to_index and next_tag in tag_to_index:\n",
    "            A[tag_to_index[prev_tag], tag_to_index[next_tag]] = count\n",
    "            \n",
    "    return A, ngrams,all_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      <e>  <s>    a  cat  dog    i   is  like  this\n",
      "<e>   0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0\n",
      "<s>   0.0  2.0  0.0  0.0  0.0  1.0  0.0   0.0   1.0\n",
      "a     0.0  0.0  0.0  2.0  0.0  0.0  0.0   0.0   0.0\n",
      "cat   2.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0\n",
      "dog   0.0  0.0  0.0  0.0  0.0  0.0  1.0   0.0   0.0\n",
      "i     0.0  0.0  0.0  0.0  0.0  0.0  0.0   1.0   0.0\n",
      "is    0.0  0.0  0.0  0.0  0.0  0.0  0.0   1.0   0.0\n",
      "like  0.0  0.0  2.0  0.0  0.0  0.0  0.0   0.0   0.0\n",
      "this  0.0  0.0  0.0  0.0  1.0  0.0  0.0   0.0   0.0\n"
     ]
    }
   ],
   "source": [
    "A,ngrams,all_tag=create_count_matrix(sentences, 2)\n",
    "keys = all_tag\n",
    "df = pd.DataFrame(A, index=keys, columns=keys)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<e>', '<s>', 'a', 'cat', 'dog', 'i', 'is', 'like', 'this']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_tag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[43mall_tag\u001b[49m\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(A, index\u001b[38;5;241m=\u001b[39mkeys, columns\u001b[38;5;241m=\u001b[39mkeys)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_tag' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_matrix(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    ngrams=count_n_gram(data, n)\n",
    "\n",
    "    \n",
    "    all_tag=sorted(ngrams.keys())\n",
    "    numtag=len(all_tag)\n",
    "    A=np.zeros((numtag,numtag))\n",
    "    for k in range(numtag):\n",
    "        for m in range(numtag):\n",
    "\n",
    "            count=0\n",
    "            keys=(all_tag[k],all_tag[m])\n",
    "            print(keys)\n",
    "            if keys in ngrams.keys():\n",
    "                count=ngrams[keys]\n",
    "            A[k,m]=count\n",
    "    return A , ngrams\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([('<s>', '<s>'), ('<s>', 'i'), ('i', 'like'), ('like', 'a'), ('a', 'cat'), ('cat', '<e>'), ('<s>', 'this'), ('this', 'dog'), ('dog', 'is'), ('is', 'like')])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts=count_n_gram(sentences, 2)\n",
    "dicts.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<s>', '<s>'): 2,\n",
       " ('<s>', 'i'): 1,\n",
       " ('i', 'like'): 1,\n",
       " ('like', 'a'): 2,\n",
       " ('a', 'cat'): 2,\n",
       " ('cat', '<e>'): 2,\n",
       " ('<s>', 'this'): 1,\n",
       " ('this', 'dog'): 1,\n",
       " ('dog', 'is'): 1,\n",
       " ('is', 'like'): 1}"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('<s>', '<s>'), ('<s>', '<s>'))\n",
      "(('<s>', '<s>'), ('<s>', 'i'))\n",
      "(('<s>', '<s>'), ('<s>', 'this'))\n",
      "(('<s>', '<s>'), ('a', 'cat'))\n",
      "(('<s>', '<s>'), ('cat', '<e>'))\n",
      "(('<s>', '<s>'), ('dog', 'is'))\n",
      "(('<s>', '<s>'), ('i', 'like'))\n",
      "(('<s>', '<s>'), ('is', 'like'))\n",
      "(('<s>', '<s>'), ('like', 'a'))\n",
      "(('<s>', '<s>'), ('this', 'dog'))\n",
      "(('<s>', 'i'), ('<s>', '<s>'))\n",
      "(('<s>', 'i'), ('<s>', 'i'))\n",
      "(('<s>', 'i'), ('<s>', 'this'))\n",
      "(('<s>', 'i'), ('a', 'cat'))\n",
      "(('<s>', 'i'), ('cat', '<e>'))\n",
      "(('<s>', 'i'), ('dog', 'is'))\n",
      "(('<s>', 'i'), ('i', 'like'))\n",
      "(('<s>', 'i'), ('is', 'like'))\n",
      "(('<s>', 'i'), ('like', 'a'))\n",
      "(('<s>', 'i'), ('this', 'dog'))\n",
      "(('<s>', 'this'), ('<s>', '<s>'))\n",
      "(('<s>', 'this'), ('<s>', 'i'))\n",
      "(('<s>', 'this'), ('<s>', 'this'))\n",
      "(('<s>', 'this'), ('a', 'cat'))\n",
      "(('<s>', 'this'), ('cat', '<e>'))\n",
      "(('<s>', 'this'), ('dog', 'is'))\n",
      "(('<s>', 'this'), ('i', 'like'))\n",
      "(('<s>', 'this'), ('is', 'like'))\n",
      "(('<s>', 'this'), ('like', 'a'))\n",
      "(('<s>', 'this'), ('this', 'dog'))\n",
      "(('a', 'cat'), ('<s>', '<s>'))\n",
      "(('a', 'cat'), ('<s>', 'i'))\n",
      "(('a', 'cat'), ('<s>', 'this'))\n",
      "(('a', 'cat'), ('a', 'cat'))\n",
      "(('a', 'cat'), ('cat', '<e>'))\n",
      "(('a', 'cat'), ('dog', 'is'))\n",
      "(('a', 'cat'), ('i', 'like'))\n",
      "(('a', 'cat'), ('is', 'like'))\n",
      "(('a', 'cat'), ('like', 'a'))\n",
      "(('a', 'cat'), ('this', 'dog'))\n",
      "(('cat', '<e>'), ('<s>', '<s>'))\n",
      "(('cat', '<e>'), ('<s>', 'i'))\n",
      "(('cat', '<e>'), ('<s>', 'this'))\n",
      "(('cat', '<e>'), ('a', 'cat'))\n",
      "(('cat', '<e>'), ('cat', '<e>'))\n",
      "(('cat', '<e>'), ('dog', 'is'))\n",
      "(('cat', '<e>'), ('i', 'like'))\n",
      "(('cat', '<e>'), ('is', 'like'))\n",
      "(('cat', '<e>'), ('like', 'a'))\n",
      "(('cat', '<e>'), ('this', 'dog'))\n",
      "(('dog', 'is'), ('<s>', '<s>'))\n",
      "(('dog', 'is'), ('<s>', 'i'))\n",
      "(('dog', 'is'), ('<s>', 'this'))\n",
      "(('dog', 'is'), ('a', 'cat'))\n",
      "(('dog', 'is'), ('cat', '<e>'))\n",
      "(('dog', 'is'), ('dog', 'is'))\n",
      "(('dog', 'is'), ('i', 'like'))\n",
      "(('dog', 'is'), ('is', 'like'))\n",
      "(('dog', 'is'), ('like', 'a'))\n",
      "(('dog', 'is'), ('this', 'dog'))\n",
      "(('i', 'like'), ('<s>', '<s>'))\n",
      "(('i', 'like'), ('<s>', 'i'))\n",
      "(('i', 'like'), ('<s>', 'this'))\n",
      "(('i', 'like'), ('a', 'cat'))\n",
      "(('i', 'like'), ('cat', '<e>'))\n",
      "(('i', 'like'), ('dog', 'is'))\n",
      "(('i', 'like'), ('i', 'like'))\n",
      "(('i', 'like'), ('is', 'like'))\n",
      "(('i', 'like'), ('like', 'a'))\n",
      "(('i', 'like'), ('this', 'dog'))\n",
      "(('is', 'like'), ('<s>', '<s>'))\n",
      "(('is', 'like'), ('<s>', 'i'))\n",
      "(('is', 'like'), ('<s>', 'this'))\n",
      "(('is', 'like'), ('a', 'cat'))\n",
      "(('is', 'like'), ('cat', '<e>'))\n",
      "(('is', 'like'), ('dog', 'is'))\n",
      "(('is', 'like'), ('i', 'like'))\n",
      "(('is', 'like'), ('is', 'like'))\n",
      "(('is', 'like'), ('like', 'a'))\n",
      "(('is', 'like'), ('this', 'dog'))\n",
      "(('like', 'a'), ('<s>', '<s>'))\n",
      "(('like', 'a'), ('<s>', 'i'))\n",
      "(('like', 'a'), ('<s>', 'this'))\n",
      "(('like', 'a'), ('a', 'cat'))\n",
      "(('like', 'a'), ('cat', '<e>'))\n",
      "(('like', 'a'), ('dog', 'is'))\n",
      "(('like', 'a'), ('i', 'like'))\n",
      "(('like', 'a'), ('is', 'like'))\n",
      "(('like', 'a'), ('like', 'a'))\n",
      "(('like', 'a'), ('this', 'dog'))\n",
      "(('this', 'dog'), ('<s>', '<s>'))\n",
      "(('this', 'dog'), ('<s>', 'i'))\n",
      "(('this', 'dog'), ('<s>', 'this'))\n",
      "(('this', 'dog'), ('a', 'cat'))\n",
      "(('this', 'dog'), ('cat', '<e>'))\n",
      "(('this', 'dog'), ('dog', 'is'))\n",
      "(('this', 'dog'), ('i', 'like'))\n",
      "(('this', 'dog'), ('is', 'like'))\n",
      "(('this', 'dog'), ('like', 'a'))\n",
      "(('this', 'dog'), ('this', 'dog'))\n"
     ]
    }
   ],
   "source": [
    "matrix,ngarms=create_count_matrix(sentences,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<s>', '<s>'): 2,\n",
       " ('<s>', 'i'): 1,\n",
       " ('i', 'like'): 1,\n",
       " ('like', 'a'): 2,\n",
       " ('a', 'cat'): 2,\n",
       " ('cat', '<e>'): 2,\n",
       " ('<s>', 'this'): 1,\n",
       " ('this', 'dog'): 1,\n",
       " ('dog', 'is'): 1,\n",
       " ('is', 'like'): 1}"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['i', 'like', 'a', 'cat'],)\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat']]\n",
    "sentence = tuple(sentences)\n",
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>',)\n",
      "('i',)\n",
      "('like',)\n",
      "('a',)\n",
      "('cat',)\n",
      "('<e>',)\n",
      "('<s>',)\n",
      "('this',)\n",
      "('dog',)\n",
      "('is',)\n",
      "('like',)\n",
      "('a',)\n",
      "('cat',)\n",
      "('<e>',)\n"
     ]
    }
   ],
   "source": [
    "create_count_matrix(sentences, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prob(data,n):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
